{"cells":[{"cell_type":"code","source":["dbutils.fs.help('mount')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ab404a59-7c7f-48c5-bb5d-6e980778be31","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class = \"ansiout\">/**<br /> * Mounts the given source directory into DBFS at the given mount point.<br /> * <br /> * Examples:<br /> *   mount(\"s3n://ACCESS_KEY:SECRET_KEY@my-twitter-bucket/tweets2013/\", \"/mnt/tweets\")<br /> *   ls(\"/mnt/tweets\")<br /> * <br /> *   mount(\"s3n://ACCESS_KEY:SECRET_KEY@my-twitter-bucket/tweets2013/\", \"/mnt/tweets\",<br /> *         \"sse-s3\")<br /> * <br /> * Mount points are persistent -- they will not be lost upon cluster or instance termination.<br /> * The mount point metadata will remain until termination of your Databricks service<br /> * (or until the point is explicitly unmounted). Once a directory is mounted, it can be<br /> * treated like a normal DBFS directory.<br /> * <br /> * Mounting a directory will securely persist any provided credentials, enabling access<br /> * to the data within the mounted directory without having to re-provide credentials.<br /> * It is not possible to access or view the credentials used to support a mount point after<br /> * the mount point is created and the command used to mount the data has been removed. Thus,<br /> * one Databricks user can mount a bucket, delete the mount command and share the data<br /> * with other Databricks users in the same organization, without sharing the security<br /> * credentials with them.<br /> * <br /> * Mounting with encryption is possible. Currently, SSE-S3 and SSE-KMS are supported. Use<br /> * encryptionType = \"sse-s3\" for sse-s3 encryption, \"sse-kms\" for sse-kms encryption with<br /> * default kms master key, and \"sse-kms:key-id\" for sse-kms encryption with separate kms key.<br /> * The source directory will not be mounted if an invalid encryption type is passed in.<br /> * <br /> * Once this method returns, the mount should be accessible from every instance within your<br /> * shard. However, since this information may be cached, you may have to run refreshMounts()<br /> * in a cluster for it to show up. Note that mount() actually runs refreshMounts() on the<br /> * current cluster.<br /> * <br /> * @param source FileSystem URI that contains the source data.<br /> *               This cannot be a DBFS URI.<br /> * @param mountPoint The directory within DBFS to mount the source. This must be within /mnt.<br /> * @param encryptionType Encryption type with which we mount the source. This means every new<br /> *                       object written using this mount will be written with encryption.<br /> * @param owner Deprecated. This parameter is deprecated, please do not set it.<br /> * @param extraConfigs A map containing extra configurations that will be used when<br /> *                     accessing the mount point. For every entry in the map, key<br /> *                     is the config name and the value is the value of the config.<br /> *                     Please only provide configs that are advised by Databricks<br /> *                     documentations.<br /> * @return True if the path was successfully mounted.<br /> */<br /><b>mount(source: java.lang.String, mountPoint: java.lang.String, encryptionType: java.lang.String = \"\", owner: java.lang.String = null, extraConfigs: scala.collection.Map = Map.empty[String, String]): boolean</b></div><br />","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div class = \"ansiout\">/**<br /> * Mounts the given source directory into DBFS at the given mount point.<br /> * <br /> * Examples:<br /> *   mount(\"s3n://ACCESS_KEY:SECRET_KEY@my-twitter-bucket/tweets2013/\", \"/mnt/tweets\")<br /> *   ls(\"/mnt/tweets\")<br /> * <br /> *   mount(\"s3n://ACCESS_KEY:SECRET_KEY@my-twitter-bucket/tweets2013/\", \"/mnt/tweets\",<br /> *         \"sse-s3\")<br /> * <br /> * Mount points are persistent -- they will not be lost upon cluster or instance termination.<br /> * The mount point metadata will remain until termination of your Databricks service<br /> * (or until the point is explicitly unmounted). Once a directory is mounted, it can be<br /> * treated like a normal DBFS directory.<br /> * <br /> * Mounting a directory will securely persist any provided credentials, enabling access<br /> * to the data within the mounted directory without having to re-provide credentials.<br /> * It is not possible to access or view the credentials used to support a mount point after<br /> * the mount point is created and the command used to mount the data has been removed. Thus,<br /> * one Databricks user can mount a bucket, delete the mount command and share the data<br /> * with other Databricks users in the same organization, without sharing the security<br /> * credentials with them.<br /> * <br /> * Mounting with encryption is possible. Currently, SSE-S3 and SSE-KMS are supported. Use<br /> * encryptionType = \"sse-s3\" for sse-s3 encryption, \"sse-kms\" for sse-kms encryption with<br /> * default kms master key, and \"sse-kms:key-id\" for sse-kms encryption with separate kms key.<br /> * The source directory will not be mounted if an invalid encryption type is passed in.<br /> * <br /> * Once this method returns, the mount should be accessible from every instance within your<br /> * shard. However, since this information may be cached, you may have to run refreshMounts()<br /> * in a cluster for it to show up. Note that mount() actually runs refreshMounts() on the<br /> * current cluster.<br /> * <br /> * @param source FileSystem URI that contains the source data.<br /> *               This cannot be a DBFS URI.<br /> * @param mountPoint The directory within DBFS to mount the source. This must be within /mnt.<br /> * @param encryptionType Encryption type with which we mount the source. This means every new<br /> *                       object written using this mount will be written with encryption.<br /> * @param owner Deprecated. This parameter is deprecated, please do not set it.<br /> * @param extraConfigs A map containing extra configurations that will be used when<br /> *                     accessing the mount point. For every entry in the map, key<br /> *                     is the config name and the value is the value of the config.<br /> *                     Please only provide configs that are advised by Databricks<br /> *                     documentations.<br /> * @return True if the path was successfully mounted.<br /> */<br /><b>mount(source: java.lang.String, mountPoint: java.lang.String, encryptionType: java.lang.String = \"\", owner: java.lang.String = null, extraConfigs: scala.collection.Map = Map.empty[String, String]): boolean</b></div><br />"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.mount(source='wasbs://source@databricksblobstorage1.blob.core.windows.net/',\n                 mount_point = '/mnt/blobstorage1',\n                extra_configs = {'fs.azure.account.key.databricksblobstorage1.blob.core.windows.net':'hVAufMZ+Gl2NyF5LlB87op7IJVy9EZTSCBn6TNeMN0Q5GwaUH0dMppcMgWigFPJShlhdTcuRVz1f+AStYh5WbQ=='})"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"8b93c7b6-fb01-441e-a9af-ea0cc0fdf3d0","inputWidgets":{},"title":"Creating Mount point from BLOB Storage using Account Key"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[2]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[2]: True"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.ls('/mnt/blobstorage1')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"d6eab6fc-81af-4546-83f5-3adefe2e2002","inputWidgets":{},"title":"List Down all the content of Mount Point"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[3]: [FileInfo(path='dbfs:/mnt/blobstorage1/Book1.csv', name='Book1.csv', size=68, modificationTime=1665636235000),\n FileInfo(path='dbfs:/mnt/blobstorage1/Demo_2.xml', name='Demo_2.xml', size=1474, modificationTime=1665636235000),\n FileInfo(path='dbfs:/mnt/blobstorage1/Fillna.csv', name='Fillna.csv', size=39, modificationTime=1665636235000),\n FileInfo(path='dbfs:/mnt/blobstorage1/Insert_file.csv', name='Insert_file.csv', size=29, modificationTime=1665636235000),\n FileInfo(path='dbfs:/mnt/blobstorage1/Testing_output_fromdatabricks/', name='Testing_output_fromdatabricks/', size=0, modificationTime=1665720911000),\n FileInfo(path='dbfs:/mnt/blobstorage1/data/', name='data/', size=0, modificationTime=1673338228000),\n FileInfo(path='dbfs:/mnt/blobstorage1/demo.xml', name='demo.xml', size=233, modificationTime=1665636235000)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[3]: [FileInfo(path='dbfs:/mnt/blobstorage1/Book1.csv', name='Book1.csv', size=68, modificationTime=1665636235000),\n FileInfo(path='dbfs:/mnt/blobstorage1/Demo_2.xml', name='Demo_2.xml', size=1474, modificationTime=1665636235000),\n FileInfo(path='dbfs:/mnt/blobstorage1/Fillna.csv', name='Fillna.csv', size=39, modificationTime=1665636235000),\n FileInfo(path='dbfs:/mnt/blobstorage1/Insert_file.csv', name='Insert_file.csv', size=29, modificationTime=1665636235000),\n FileInfo(path='dbfs:/mnt/blobstorage1/Testing_output_fromdatabricks/', name='Testing_output_fromdatabricks/', size=0, modificationTime=1665720911000),\n FileInfo(path='dbfs:/mnt/blobstorage1/data/', name='data/', size=0, modificationTime=1673338228000),\n FileInfo(path='dbfs:/mnt/blobstorage1/demo.xml', name='demo.xml', size=233, modificationTime=1665636235000)]"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp('/mnt/blobstorage1/data/Book1.csv','/mnt/blobstorage1/output/Book1.csv')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"071effb0-91c9-4aea-8459-eb09f2da21d5","inputWidgets":{},"title":"Copy Operation inside Mount Point"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[4]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[4]: True"]}}],"execution_count":0},{"cell_type":"code","source":["df=spark.read.csv('/mnt/blobstorage1/Insert_file.csv',header=True)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"1602d36d-d6e9-4bec-bc35-d6ee012d0c39","inputWidgets":{},"title":"Reading the CSV file from Mount Point"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----+\n| Id|Name|\n+---+----+\n|  1|   A|\n|  2|   B|\n|  3|   C|\n|  4|   D|\n+---+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----+\n| Id|Name|\n+---+----+\n|  1|   A|\n|  2|   B|\n|  3|   C|\n|  4|   D|\n+---+----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.mount(source='wasbs://source@databricksblobstorage1.blob.core.windows.net/',\n                 mount_point = '/mnt/blobstorage2',\n                extra_configs = {'fs.azure.sas.source.databricksblobstorage1.blob.core.windows.net':'sv=2021-06-08&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2023-01-11T13:23:11Z&st=2023-01-11T05:23:11Z&spr=https&sig=JFBiNgS0TFLSpNWiXvPwTwcJ2e6DWZQ97RyU83T2YrM%3D'})"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"4f3b3246-299c-4962-98ad-34a4675b515e","inputWidgets":{},"title":"Creating Mount point from BLOB Storage using SAS Key"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[5]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[5]: True"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.ls('/mnt/blobstorage2/data/')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"abfd9571-0758-4543-bc1e-4de03d6a46a0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[7]: [FileInfo(path='dbfs:/mnt/blobstorage2/data/Book1.csv', name='Book1.csv', size=68, modificationTime=1673338228000)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[7]: [FileInfo(path='dbfs:/mnt/blobstorage2/data/Book1.csv', name='Book1.csv', size=68, modificationTime=1673338228000)]"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.help()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"55f2705b-881d-4675-82d6-b3f0bc24ce40","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\nthis package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\nanother FileSystem URI.\n\nFor more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n\nIn notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\nstraightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\ntranslates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n    <h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /><h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><b>updateMount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Similar to mount(), but updates an existing mount point (if present) instead of creating a new one<br /><br /></div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\nthis package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\nanother FileSystem URI.\n\nFor more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n\nIn notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\nstraightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\ntranslates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n    <h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /><h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><b>updateMount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Similar to mount(), but updates an existing mount point (if present) instead of creating a new one<br /><br /></div>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.help('unmount')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25b68d8d-1cea-48e7-92b8-a706f572dc21","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class = \"ansiout\">/**<br /> * Deletes a DBFS mount point. Once this method returns, the mount point metadata is<br /> * guaranteed to be deleted from persistent storage and should be inaccessible from every<br /> * instance within your shard. However, since this information may be cached, you may have to<br /> * run refreshMounts() in a cluster for it to disappear. Note that unmount() actually runs<br /> * refreshMounts() on the current cluster.<br /> * <br /> * Example:<br /> *   unmount(\"/mnt/tweets\")<br /> * <br /> * @param mountPoint DBFS directory previously mounted<br /> * @return True if the mount point was successfully unmounted, or wasn't mounted originally.<br /> */<br /><b>unmount(mountPoint: java.lang.String): boolean</b></div><br />","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div class = \"ansiout\">/**<br /> * Deletes a DBFS mount point. Once this method returns, the mount point metadata is<br /> * guaranteed to be deleted from persistent storage and should be inaccessible from every<br /> * instance within your shard. However, since this information may be cached, you may have to<br /> * run refreshMounts() in a cluster for it to disappear. Note that unmount() actually runs<br /> * refreshMounts() on the current cluster.<br /> * <br /> * Example:<br /> *   unmount(\"/mnt/tweets\")<br /> * <br /> * @param mountPoint DBFS directory previously mounted<br /> * @return True if the mount point was successfully unmounted, or wasn't mounted originally.<br /> */<br /><b>unmount(mountPoint: java.lang.String): boolean</b></div><br />"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.unmount('/mnt/blobstorage2')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"663c7cb0-01e1-4b82-a999-849c396d8506","inputWidgets":{},"title":"Deleting the Mount point"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/mnt/blobstorage2 has been unmounted.\nOut[4]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/mnt/blobstorage2 has been unmounted.\nOut[4]: True"]}}],"execution_count":0},{"cell_type":"code","source":["df=spark.read.csv('/mnt/blobstorage2/Insert_file.csv',header=True)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"e73737e0-2722-4f11-a7c1-063314df88cf","inputWidgets":{},"title":"No Able to read the file since the Mount point was unmounted"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-318369083857565>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'/mnt/blobstorage2/Insert_file.csv'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mcsv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n\u001B[1;32m    533\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 535\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    536\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path does not exist: dbfs:/mnt/blobstorage2/Insert_file.csv","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Path does not exist: dbfs:/mnt/blobstorage2/Insert_file.csv","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-318369083857565>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'/mnt/blobstorage2/Insert_file.csv'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mcsv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n\u001B[1;32m    533\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 535\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    536\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path does not exist: dbfs:/mnt/blobstorage2/Insert_file.csv"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.help('mounts')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fcea105f-667e-427b-8eb6-4b655ebb1947","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class = \"ansiout\">/**<br /> * Displays information about what is mounted within DBFS. The returned information includes<br /> * the mount point, source directory, and encryption type. Any credentials used to mount the<br /> * mount points listed will not be displayed.<br /> * <br /> * Example:<br /> *   display(mounts())<br /> * <br /> * @return Ordered sequence of MountInfos containing the mount point, the source of every<br /> *         mount, and the encryption type of that mount, if present.<br /> */<br /><b>mounts: scala.collection.Seq</b></div><br />","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div class = \"ansiout\">/**<br /> * Displays information about what is mounted within DBFS. The returned information includes<br /> * the mount point, source directory, and encryption type. Any credentials used to mount the<br /> * mount points listed will not be displayed.<br /> * <br /> * Example:<br /> *   display(mounts())<br /> * <br /> * @return Ordered sequence of MountInfos containing the mount point, the source of every<br /> *         mount, and the encryption type of that mount, if present.<br /> */<br /><b>mounts: scala.collection.Seq</b></div><br />"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.mounts()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"dc1fb806-bfe5-4d0d-9718-713212e50675","inputWidgets":{},"title":"To See all the Mount points in this DBFS"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[7]: [MountInfo(mountPoint='/databricks-datasets', source='databricks-datasets', encryptionType=''),\n MountInfo(mountPoint='/databricks/mlflow-tracking', source='databricks/mlflow-tracking', encryptionType=''),\n MountInfo(mountPoint='/databricks-results', source='databricks-results', encryptionType=''),\n MountInfo(mountPoint='/databricks/mlflow-registry', source='databricks/mlflow-registry', encryptionType=''),\n MountInfo(mountPoint='/mnt/blobstorage1', source='wasbs://source@databricksblobstorage1.blob.core.windows.net/', encryptionType=''),\n MountInfo(mountPoint='/mnt/blobstorage', source='wasbs://source@databricksblobstorage1.blob.core.windows.net/', encryptionType=''),\n MountInfo(mountPoint='/', source='DatabricksRoot', encryptionType='')]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[7]: [MountInfo(mountPoint='/databricks-datasets', source='databricks-datasets', encryptionType=''),\n MountInfo(mountPoint='/databricks/mlflow-tracking', source='databricks/mlflow-tracking', encryptionType=''),\n MountInfo(mountPoint='/databricks-results', source='databricks-results', encryptionType=''),\n MountInfo(mountPoint='/databricks/mlflow-registry', source='databricks/mlflow-registry', encryptionType=''),\n MountInfo(mountPoint='/mnt/blobstorage1', source='wasbs://source@databricksblobstorage1.blob.core.windows.net/', encryptionType=''),\n MountInfo(mountPoint='/mnt/blobstorage', source='wasbs://source@databricksblobstorage1.blob.core.windows.net/', encryptionType=''),\n MountInfo(mountPoint='/', source='DatabricksRoot', encryptionType='')]"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.help('refreshMounts')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8322b29f-37aa-414d-920f-a9753a8c7fe5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class = \"ansiout\">/**<br /> * Forces all machines in this cluster to refresh their mount cache, ensuring they receive<br /> * the most recent information.<br /> * <br /> * You may have to call this method if you mount or unmount a directory inside one cluster,<br /> * and then quickly switch to another cluster (where it may be cached but not updated).<br /> * <br /> * Creating or deleting a mount automatically calls this method in the current cluster.<br /> * @return True if the mount points were refreshed successfully.<br /> */<br /><b>refreshMounts: boolean</b></div><br />","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div class = \"ansiout\">/**<br /> * Forces all machines in this cluster to refresh their mount cache, ensuring they receive<br /> * the most recent information.<br /> * <br /> * You may have to call this method if you mount or unmount a directory inside one cluster,<br /> * and then quickly switch to another cluster (where it may be cached but not updated).<br /> * <br /> * Creating or deleting a mount automatically calls this method in the current cluster.<br /> * @return True if the mount points were refreshed successfully.<br /> */<br /><b>refreshMounts: boolean</b></div><br />"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.refreshMounts()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"e60e097c-9adc-45d7-a054-f8cdc07648d5","inputWidgets":{},"title":"To Refresh all the Mount Cache to receive the most recent information from Mount point"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Mounts successfully refreshed.\nOut[9]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Mounts successfully refreshed.\nOut[9]: True"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.help('updateMount')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2146c1bb-ba1a-410f-986d-5eecb073a157","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class = \"ansiout\">/**<br /> * Similar to mount(), but updates an existing mount point (if present) instead of creating<br /> * a new one. If there is no mount point defined at the path specified by the mountPoint<br /> * parameter, the method will throw an error.<br /> * <br /> * Example:<br /> *   updateMount(\"s3n://ACCESS_KEY:SECRET_KEY@my-twitter-bucket/tweets2013/\", \"/mnt/tweets\")<br /> *   => succeeds if a mount point is already defined at \"/mnt/tweets\". Throws error otherwise.<br /> * <br /> * @param source FileSystem URI that contains the source data.<br /> *               This cannot be a DBFS URI.<br /> * @param mountPoint The directory within DBFS which represents an existing mount point.<br /> * @param encryptionType Encryption type with which we mount the source. This means every new<br /> *                       object written using this mount will be written with encryption.<br /> * @param owner Deprecated. This parameter is deprecated, please do not set it.<br /> * @param extraConfigs A map containing extra configurations that will be used when<br /> *                     accessing the mount point. For every entry in the map, key<br /> *                     is the config name and the value is the value of the config.<br /> *                     Please only provide configs that are advised by Databricks<br /> *                     documentations.<br /> * @return True if the mount point was successfully updated.<br /> */<br /><b>updateMount(source: java.lang.String, mountPoint: java.lang.String, encryptionType: java.lang.String = \"\", owner: java.lang.String = null, extraConfigs: scala.collection.Map = Map.empty[String, String]): boolean</b></div><br />","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div class = \"ansiout\">/**<br /> * Similar to mount(), but updates an existing mount point (if present) instead of creating<br /> * a new one. If there is no mount point defined at the path specified by the mountPoint<br /> * parameter, the method will throw an error.<br /> * <br /> * Example:<br /> *   updateMount(\"s3n://ACCESS_KEY:SECRET_KEY@my-twitter-bucket/tweets2013/\", \"/mnt/tweets\")<br /> *   => succeeds if a mount point is already defined at \"/mnt/tweets\". Throws error otherwise.<br /> * <br /> * @param source FileSystem URI that contains the source data.<br /> *               This cannot be a DBFS URI.<br /> * @param mountPoint The directory within DBFS which represents an existing mount point.<br /> * @param encryptionType Encryption type with which we mount the source. This means every new<br /> *                       object written using this mount will be written with encryption.<br /> * @param owner Deprecated. This parameter is deprecated, please do not set it.<br /> * @param extraConfigs A map containing extra configurations that will be used when<br /> *                     accessing the mount point. For every entry in the map, key<br /> *                     is the config name and the value is the value of the config.<br /> *                     Please only provide configs that are advised by Databricks<br /> *                     documentations.<br /> * @return True if the mount point was successfully updated.<br /> */<br /><b>updateMount(source: java.lang.String, mountPoint: java.lang.String, encryptionType: java.lang.String = \"\", owner: java.lang.String = null, extraConfigs: scala.collection.Map = Map.empty[String, String]): boolean</b></div><br />"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.updateMount(source='wasbs://source@databricksblobstorage1.blob.core.windows.net/Testing_output_fromdatabricks',\n                 mount_point = '/mnt/blobstorage1',\n                extra_configs = {'fs.azure.account.key.databricksblobstorage1.blob.core.windows.net':'hVAufMZ+Gl2NyF5LlB87op7IJVy9EZTSCBn6TNeMN0Q5GwaUH0dMppcMgWigFPJShlhdTcuRVz1f+AStYh5WbQ=='})"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"3a59a97c-2f26-494a-9a61-56f9c246242b","inputWidgets":{},"title":"Updating Mount point with different location when mount point is exist"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[2]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[2]: True"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.ls('/mnt/blobstorage1')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b9b7ebd2-0274-4557-b242-e7c583da549f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[3]: [FileInfo(path='dbfs:/mnt/blobstorage1/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1665720912000),\n FileInfo(path='dbfs:/mnt/blobstorage1/_committed_7295972069547135001', name='_committed_7295972069547135001', size=122, modificationTime=1665720909000),\n FileInfo(path='dbfs:/mnt/blobstorage1/_started_7295972069547135001', name='_started_7295972069547135001', size=0, modificationTime=1665720904000),\n FileInfo(path='dbfs:/mnt/blobstorage1/part-00000-tid-7295972069547135001-5d3b50b8-7270-407c-a75e-ba3f71699717-3-1-c000.snappy.parquet', name='part-00000-tid-7295972069547135001-5d3b50b8-7270-407c-a75e-ba3f71699717-3-1-c000.snappy.parquet', size=998, modificationTime=1665720908000)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[3]: [FileInfo(path='dbfs:/mnt/blobstorage1/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1665720912000),\n FileInfo(path='dbfs:/mnt/blobstorage1/_committed_7295972069547135001', name='_committed_7295972069547135001', size=122, modificationTime=1665720909000),\n FileInfo(path='dbfs:/mnt/blobstorage1/_started_7295972069547135001', name='_started_7295972069547135001', size=0, modificationTime=1665720904000),\n FileInfo(path='dbfs:/mnt/blobstorage1/part-00000-tid-7295972069547135001-5d3b50b8-7270-407c-a75e-ba3f71699717-3-1-c000.snappy.parquet', name='part-00000-tid-7295972069547135001-5d3b50b8-7270-407c-a75e-ba3f71699717-3-1-c000.snappy.parquet', size=998, modificationTime=1665720908000)]"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.updateMount(source='wasbs://source@databricksblobstorage1.blob.core.windows.net/Testing_output_fromdatabricks',\n                 mount_point = '/mnt/blobstorage2',\n                extra_configs = {'fs.azure.account.key.databricksblobstorage1.blob.core.windows.net':'hVAufMZ+Gl2NyF5LlB87op7IJVy9EZTSCBn6TNeMN0Q5GwaUH0dMppcMgWigFPJShlhdTcuRVz1f+AStYh5WbQ=='})"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"032982cd-02f5-49e6-bf89-e8e0bac489bb","inputWidgets":{},"title":"Updating Mount point with different location when mount point is not exist and facing error"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-92510609274696>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m dbutils.fs.updateMount(source='wasbs://source@databricksblobstorage1.blob.core.windows.net/Testing_output_fromdatabricks',\n\u001B[0m\u001B[1;32m      2\u001B[0m                  \u001B[0mmount_point\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'/mnt/blobstorage2'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m                 extra_configs = {'fs.azure.account.key.databricksblobstorage1.blob.core.windows.net':'hVAufMZ+Gl2NyF5LlB87op7IJVy9EZTSCBn6TNeMN0Q5GwaUH0dMppcMgWigFPJShlhdTcuRVz1f+AStYh5WbQ=='})\n\n\u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py\u001B[0m in \u001B[0;36mf_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    360\u001B[0m                     \u001B[0mexc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__context__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    361\u001B[0m                     \u001B[0mexc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__cause__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 362\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mexc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    363\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    364\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mf_with_exception_handling\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o380.updateMount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Mount point does not exist: /mnt/blobstorage2; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Mount point does not exist: /mnt/blobstorage2\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1002)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$updateMount$1(DBUtilsCore.scala:1053)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:541)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:398)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:396)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:393)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:66)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:441)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:426)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:66)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:550)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:66)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:541)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:511)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:66)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:130)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.updateMount(DBUtilsCore.scala:1047)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Mount point does not exist: /mnt/blobstorage2\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$updateMount$1(MetadataManager.scala:521)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$1(MetadataManager.scala:852)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:633)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:841)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.updateMount(MetadataManager.scala:537)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:128)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:347)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:306)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:119)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:146)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:146)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:116)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:639)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:660)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:401)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:399)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:396)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:444)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:429)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:634)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:553)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:514)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:24)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:115)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:956)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:956)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:872)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2(JettyServer.scala:502)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2$adapted(JettyServer.scala:477)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:372)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:401)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:399)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:396)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:56)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:444)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:429)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionTags(ActivityContextFactory.scala:56)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:372)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:167)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:477)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:374)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:80)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n","errorSummary":"java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Mount point does not exist: /mnt/blobstorage2; nested exception is: ","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-92510609274696>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m dbutils.fs.updateMount(source='wasbs://source@databricksblobstorage1.blob.core.windows.net/Testing_output_fromdatabricks',\n\u001B[0m\u001B[1;32m      2\u001B[0m                  \u001B[0mmount_point\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'/mnt/blobstorage2'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m                 extra_configs = {'fs.azure.account.key.databricksblobstorage1.blob.core.windows.net':'hVAufMZ+Gl2NyF5LlB87op7IJVy9EZTSCBn6TNeMN0Q5GwaUH0dMppcMgWigFPJShlhdTcuRVz1f+AStYh5WbQ=='})\n\n\u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py\u001B[0m in \u001B[0;36mf_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    360\u001B[0m                     \u001B[0mexc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__context__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    361\u001B[0m                     \u001B[0mexc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__cause__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 362\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mexc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    363\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    364\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mf_with_exception_handling\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o380.updateMount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Mount point does not exist: /mnt/blobstorage2; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Mount point does not exist: /mnt/blobstorage2\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1002)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$updateMount$1(DBUtilsCore.scala:1053)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:541)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:398)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:396)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:393)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:66)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:441)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:426)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:66)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:550)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:66)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:541)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:511)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:66)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:130)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.updateMount(DBUtilsCore.scala:1047)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Mount point does not exist: /mnt/blobstorage2\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$updateMount$1(MetadataManager.scala:521)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$1(MetadataManager.scala:852)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:633)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:841)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.updateMount(MetadataManager.scala:537)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:128)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:347)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:306)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:119)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:146)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:146)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:116)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:639)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:660)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:401)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:399)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:396)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:444)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:429)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:634)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:553)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:514)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:24)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:115)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:956)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:956)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:872)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2(JettyServer.scala:502)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2$adapted(JettyServer.scala:477)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:372)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:401)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:399)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:396)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:56)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:444)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:429)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionTags(ActivityContextFactory.scala:56)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:372)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:167)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:477)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:374)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:80)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.help()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"73c7ed69-1eb4-4c03-950a-adb680d6c104","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class = \"ansiout\">\nThis module provides various utilities for users to interact with the rest of Databricks.\n  <h3></h3><b>credentials: DatabricksCredentialUtils</b> -> Utilities for interacting with credentials within notebooks<br /><b>data: DataUtils</b> -> Utilities for understanding and interacting with datasets (EXPERIMENTAL)<br /><b>fs: DbfsUtils</b> -> Manipulates the Databricks filesystem (DBFS) from the console<br /><b>jobs: JobsUtils</b> -> Utilities for leveraging jobs features<br /><b>library: LibraryUtils</b> -> Utilities for session isolated libraries<br /><b>meta: MetaUtils</b> -> Methods to hook into the compiler (EXPERIMENTAL)<br /><b>notebook: NotebookUtils</b> -> Utilities for the control flow of a notebook (EXPERIMENTAL)<br /><b>preview: Preview</b> -> Utilities under preview category<br /><b>secrets: SecretUtils</b> -> Provides utilities for leveraging secrets within notebooks<br /><b>widgets: WidgetsUtils</b> -> Methods to create and get bound value of input widgets inside notebooks<br /><br /></div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div class = \"ansiout\">\nThis module provides various utilities for users to interact with the rest of Databricks.\n  <h3></h3><b>credentials: DatabricksCredentialUtils</b> -> Utilities for interacting with credentials within notebooks<br /><b>data: DataUtils</b> -> Utilities for understanding and interacting with datasets (EXPERIMENTAL)<br /><b>fs: DbfsUtils</b> -> Manipulates the Databricks filesystem (DBFS) from the console<br /><b>jobs: JobsUtils</b> -> Utilities for leveraging jobs features<br /><b>library: LibraryUtils</b> -> Utilities for session isolated libraries<br /><b>meta: MetaUtils</b> -> Methods to hook into the compiler (EXPERIMENTAL)<br /><b>notebook: NotebookUtils</b> -> Utilities for the control flow of a notebook (EXPERIMENTAL)<br /><b>preview: Preview</b> -> Utilities under preview category<br /><b>secrets: SecretUtils</b> -> Provides utilities for leveraging secrets within notebooks<br /><b>widgets: WidgetsUtils</b> -> Methods to create and get bound value of input widgets inside notebooks<br /><br /></div>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.secrets.help()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0d740220-ee1c-4a47-8d51-8ead155b0cbd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class = \"ansiout\">\nProvides utilities for leveraging secrets within notebooks.\nDatabricks documentation for more info.\n    <h3></h3><b>get(scope: String, key: String): String</b> -> Gets the string representation of a secret value with scope and key<br /><b>getBytes(scope: String, key: String): byte[]</b> -> Gets the bytes representation of a secret value with scope and key<br /><b>list(scope: String): Seq</b> -> Lists secret metadata for secrets within a scope<br /><b>listScopes: Seq</b> -> Lists secret scopes<br /><br /></div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div class = \"ansiout\">\nProvides utilities for leveraging secrets within notebooks.\nDatabricks documentation for more info.\n    <h3></h3><b>get(scope: String, key: String): String</b> -> Gets the string representation of a secret value with scope and key<br /><b>getBytes(scope: String, key: String): byte[]</b> -> Gets the bytes representation of a secret value with scope and key<br /><b>list(scope: String): Seq</b> -> Lists secret metadata for secrets within a scope<br /><b>listScopes: Seq</b> -> Lists secret scopes<br /><br /></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc1e69a7-7e6c-4142-b40a-428256aff6e3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"TestingMount_Notebook","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2013553742570656}},"nbformat":4,"nbformat_minor":0}
