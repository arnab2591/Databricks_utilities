{"cells":[{"cell_type":"code","source":["dbutils.help()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"30d6063b-b056-4f74-b5ad-61cbcedf79f9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class = \"ansiout\">\nThis module provides various utilities for users to interact with the rest of Databricks.\n  <h3></h3><b>credentials: DatabricksCredentialUtils</b> -> Utilities for interacting with credentials within notebooks<br /><b>data: DataUtils</b> -> Utilities for understanding and interacting with datasets (EXPERIMENTAL)<br /><b>fs: DbfsUtils</b> -> Manipulates the Databricks filesystem (DBFS) from the console<br /><b>jobs: JobsUtils</b> -> Utilities for leveraging jobs features<br /><b>library: LibraryUtils</b> -> Utilities for session isolated libraries<br /><b>meta: MetaUtils</b> -> Methods to hook into the compiler (EXPERIMENTAL)<br /><b>notebook: NotebookUtils</b> -> Utilities for the control flow of a notebook (EXPERIMENTAL)<br /><b>preview: Preview</b> -> Utilities under preview category<br /><b>secrets: SecretUtils</b> -> Provides utilities for leveraging secrets within notebooks<br /><b>widgets: WidgetsUtils</b> -> Methods to create and get bound value of input widgets inside notebooks<br /><br /></div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div class = \"ansiout\">\nThis module provides various utilities for users to interact with the rest of Databricks.\n  <h3></h3><b>credentials: DatabricksCredentialUtils</b> -> Utilities for interacting with credentials within notebooks<br /><b>data: DataUtils</b> -> Utilities for understanding and interacting with datasets (EXPERIMENTAL)<br /><b>fs: DbfsUtils</b> -> Manipulates the Databricks filesystem (DBFS) from the console<br /><b>jobs: JobsUtils</b> -> Utilities for leveraging jobs features<br /><b>library: LibraryUtils</b> -> Utilities for session isolated libraries<br /><b>meta: MetaUtils</b> -> Methods to hook into the compiler (EXPERIMENTAL)<br /><b>notebook: NotebookUtils</b> -> Utilities for the control flow of a notebook (EXPERIMENTAL)<br /><b>preview: Preview</b> -> Utilities under preview category<br /><b>secrets: SecretUtils</b> -> Provides utilities for leveraging secrets within notebooks<br /><b>widgets: WidgetsUtils</b> -> Methods to create and get bound value of input widgets inside notebooks<br /><br /></div>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.help()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c82dc211-ccdf-409f-89b4-4b7e91bcfd41","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\nthis package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\nanother FileSystem URI.\n\nFor more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n\nIn notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\nstraightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\ntranslates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n    <h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><b>updateMount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Similar to mount(), but updates an existing mount point (if present) instead of creating a new one<br /><br /><h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /></div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\nthis package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\nanother FileSystem URI.\n\nFor more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n\nIn notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\nstraightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\ntranslates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n    <h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><b>updateMount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Similar to mount(), but updates an existing mount point (if present) instead of creating a new one<br /><br /><h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /></div>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.help('mount')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"82b63a81-451d-418f-89a2-71c88b06f4d8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class = \"ansiout\">/**<br /> * Mounts the given source directory into DBFS at the given mount point.<br /> * <br /> * Examples:<br /> *   mount(\"s3n://ACCESS_KEY:SECRET_KEY@my-twitter-bucket/tweets2013/\", \"/mnt/tweets\")<br /> *   ls(\"/mnt/tweets\")<br /> * <br /> *   mount(\"s3n://ACCESS_KEY:SECRET_KEY@my-twitter-bucket/tweets2013/\", \"/mnt/tweets\",<br /> *         \"sse-s3\")<br /> * <br /> * Mount points are persistent -- they will not be lost upon cluster or instance termination.<br /> * The mount point metadata will remain until termination of your Databricks service<br /> * (or until the point is explicitly unmounted). Once a directory is mounted, it can be<br /> * treated like a normal DBFS directory.<br /> * <br /> * Mounting a directory will securely persist any provided credentials, enabling access<br /> * to the data within the mounted directory without having to re-provide credentials.<br /> * It is not possible to access or view the credentials used to support a mount point after<br /> * the mount point is created and the command used to mount the data has been removed. Thus,<br /> * one Databricks user can mount a bucket, delete the mount command and share the data<br /> * with other Databricks users in the same organization, without sharing the security<br /> * credentials with them.<br /> * <br /> * Mounting with encryption is possible. Currently, SSE-S3 and SSE-KMS are supported. Use<br /> * encryptionType = \"sse-s3\" for sse-s3 encryption, \"sse-kms\" for sse-kms encryption with<br /> * default kms master key, and \"sse-kms:key-id\" for sse-kms encryption with separate kms key.<br /> * The source directory will not be mounted if an invalid encryption type is passed in.<br /> * <br /> * Once this method returns, the mount should be accessible from every instance within your<br /> * shard. However, since this information may be cached, you may have to run refreshMounts()<br /> * in a cluster for it to show up. Note that mount() actually runs refreshMounts() on the<br /> * current cluster.<br /> * <br /> * @param source FileSystem URI that contains the source data.<br /> *               This cannot be a DBFS URI.<br /> * @param mountPoint The directory within DBFS to mount the source. This must be within /mnt.<br /> * @param encryptionType Encryption type with which we mount the source. This means every new<br /> *                       object written using this mount will be written with encryption.<br /> * @param owner Deprecated. This parameter is deprecated, please do not set it.<br /> * @param extraConfigs A map containing extra configurations that will be used when<br /> *                     accessing the mount point. For every entry in the map, key<br /> *                     is the config name and the value is the value of the config.<br /> *                     Please only provide configs that are advised by Databricks<br /> *                     documentations.<br /> * @return True if the path was successfully mounted.<br /> */<br /><b>mount(source: java.lang.String, mountPoint: java.lang.String, encryptionType: java.lang.String = \"\", owner: java.lang.String = null, extraConfigs: scala.collection.Map = Map.empty[String, String]): boolean</b></div><br />","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div class = \"ansiout\">/**<br /> * Mounts the given source directory into DBFS at the given mount point.<br /> * <br /> * Examples:<br /> *   mount(\"s3n://ACCESS_KEY:SECRET_KEY@my-twitter-bucket/tweets2013/\", \"/mnt/tweets\")<br /> *   ls(\"/mnt/tweets\")<br /> * <br /> *   mount(\"s3n://ACCESS_KEY:SECRET_KEY@my-twitter-bucket/tweets2013/\", \"/mnt/tweets\",<br /> *         \"sse-s3\")<br /> * <br /> * Mount points are persistent -- they will not be lost upon cluster or instance termination.<br /> * The mount point metadata will remain until termination of your Databricks service<br /> * (or until the point is explicitly unmounted). Once a directory is mounted, it can be<br /> * treated like a normal DBFS directory.<br /> * <br /> * Mounting a directory will securely persist any provided credentials, enabling access<br /> * to the data within the mounted directory without having to re-provide credentials.<br /> * It is not possible to access or view the credentials used to support a mount point after<br /> * the mount point is created and the command used to mount the data has been removed. Thus,<br /> * one Databricks user can mount a bucket, delete the mount command and share the data<br /> * with other Databricks users in the same organization, without sharing the security<br /> * credentials with them.<br /> * <br /> * Mounting with encryption is possible. Currently, SSE-S3 and SSE-KMS are supported. Use<br /> * encryptionType = \"sse-s3\" for sse-s3 encryption, \"sse-kms\" for sse-kms encryption with<br /> * default kms master key, and \"sse-kms:key-id\" for sse-kms encryption with separate kms key.<br /> * The source directory will not be mounted if an invalid encryption type is passed in.<br /> * <br /> * Once this method returns, the mount should be accessible from every instance within your<br /> * shard. However, since this information may be cached, you may have to run refreshMounts()<br /> * in a cluster for it to show up. Note that mount() actually runs refreshMounts() on the<br /> * current cluster.<br /> * <br /> * @param source FileSystem URI that contains the source data.<br /> *               This cannot be a DBFS URI.<br /> * @param mountPoint The directory within DBFS to mount the source. This must be within /mnt.<br /> * @param encryptionType Encryption type with which we mount the source. This means every new<br /> *                       object written using this mount will be written with encryption.<br /> * @param owner Deprecated. This parameter is deprecated, please do not set it.<br /> * @param extraConfigs A map containing extra configurations that will be used when<br /> *                     accessing the mount point. For every entry in the map, key<br /> *                     is the config name and the value is the value of the config.<br /> *                     Please only provide configs that are advised by Databricks<br /> *                     documentations.<br /> * @return True if the path was successfully mounted.<br /> */<br /><b>mount(source: java.lang.String, mountPoint: java.lang.String, encryptionType: java.lang.String = \"\", owner: java.lang.String = null, extraConfigs: scala.collection.Map = Map.empty[String, String]): boolean</b></div><br />"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.mount(source='wasbs://<containerName>@<accountName>.blob.core.windows.net/',\n                mount_point = '/mnt/<mountName>',\n                extra_configs = {'fs.azure.account.key.blobstorage.blob.core.windows.net':'accountkey'})"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"d144ae28-0e64-4e14-bd35-4e71cb854b27","inputWidgets":{},"title":"Syntax to mount a folder to local workspace"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.mount(source='wasbs://source@databricksblobstorage1.blob.core.windows.net/',\n                mount_point = '/mnt/blobstorage',\n                extra_configs = {'fs.azure.account.key.databricksblobstorage1.blob.core.windows.net':'hVAufMZ+Gl2NyF5LlB87op7IJVy9EZTSCBn6TNeMN0Q5GwaUH0dMppcMgWigFPJShlhdTcuRVz1f+AStYh5WbQ=='})"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"04ffee61-7299-464b-ba32-d1364c94b0fa","inputWidgets":{},"title":"Mounting A Blobstorage"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[5]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[5]: True"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.ls('/mnt/blobstorage')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"163a004e-20bd-4cea-bc82-19f0f133ea84","inputWidgets":{},"title":"List down all the folder and files from mount point"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[6]: [FileInfo(path='dbfs:/mnt/blobstorage/Book1.csv', name='Book1.csv', size=68, modificationTime=1665636235000),\n FileInfo(path='dbfs:/mnt/blobstorage/Demo_2.xml', name='Demo_2.xml', size=1474, modificationTime=1665636235000),\n FileInfo(path='dbfs:/mnt/blobstorage/Fillna.csv', name='Fillna.csv', size=39, modificationTime=1665636235000),\n FileInfo(path='dbfs:/mnt/blobstorage/Insert_file.csv', name='Insert_file.csv', size=29, modificationTime=1665636235000),\n FileInfo(path='dbfs:/mnt/blobstorage/Testing_output_fromdatabricks/', name='Testing_output_fromdatabricks/', size=0, modificationTime=1665720911000),\n FileInfo(path='dbfs:/mnt/blobstorage/demo.xml', name='demo.xml', size=233, modificationTime=1665636235000)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[6]: [FileInfo(path='dbfs:/mnt/blobstorage/Book1.csv', name='Book1.csv', size=68, modificationTime=1665636235000),\n FileInfo(path='dbfs:/mnt/blobstorage/Demo_2.xml', name='Demo_2.xml', size=1474, modificationTime=1665636235000),\n FileInfo(path='dbfs:/mnt/blobstorage/Fillna.csv', name='Fillna.csv', size=39, modificationTime=1665636235000),\n FileInfo(path='dbfs:/mnt/blobstorage/Insert_file.csv', name='Insert_file.csv', size=29, modificationTime=1665636235000),\n FileInfo(path='dbfs:/mnt/blobstorage/Testing_output_fromdatabricks/', name='Testing_output_fromdatabricks/', size=0, modificationTime=1665720911000),\n FileInfo(path='dbfs:/mnt/blobstorage/demo.xml', name='demo.xml', size=233, modificationTime=1665636235000)]"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp('/mnt/blobstorage/Book1.csv','/mnt/blobstorage/data/Book1.csv')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"26d87c95-b315-4809-9190-92657c78632a","inputWidgets":{},"title":"Copy file to different new folder in mount point which will change the folder structure in Blob storage"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[7]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[7]: True"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4829f93-24f0-422d-a0ea-627b104b8645","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"MountNotebook","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2536384037186753}},"nbformat":4,"nbformat_minor":0}
